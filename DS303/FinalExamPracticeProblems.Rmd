---
title: "Final Exam Practice Problems"
author: "Neha Maddali"
date: "2023-12-04"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Concept Review

1. True or False? Justify your answer. When carrying out a hypothesis test, as n increases, the type 1 error will also decrease
    + False, the type 1 error is affected by the significance level (alpha)

2. True or False? Justify your answer. When carrying out a hypothesis test, as n increases, the ability to reject the H0 (power) will also increase.
    + True, as n increases, it lowers the variability of the test error which increases the chances that the null hypothesis will be rejected.

3. Suppose your colleague fits a logistic regression model with all predictors and finds that roughly 30% of the predictors have non-significant p-values. He wants to drop those predictors and only keep all the remaining significant predictors. He asks for your advice. What do you tell him?
    + The non-significant predictors should not be dropped. Dropping a subset of the predictors would affect the learned relationships between predictors and response, which could lead to starkly different results when including or excluding the non-significant predictors.
    + Don't trust the p-values because of the multiple testing problem. We make assumptions to get these p-values. We need a distributional assumption to do inference.

4. True or False? Justify your answer. Since ridge regression (with λ > 0) introduces a penalty term to protect us from overfitting, its training MSE will always be smaller than that of least square regression.
    + False. Ridge regression introduces the shrinkage penalty to minimize the test MSE, not the training MSE. Therefore, this will introduce some bias which may increase the training MSE, but should result in the minimum value of test MSE. Least squares is more flexible, so its training MSE will be smaller than ridge

5. True or False? Justify your answer. As K increases, the KNN classifier because more flexible.
    + False, because as K increases, the model will actually decrease in flexibility/complexity. This is because the more observations we use to classify the less the decision boundary is affected by individual values.

6. True or False? Justify your answer. For a given data set, suppose we want to prune a decision tree. We can directly calculate the bias and variance of a tree for different sizes. Based on this, we can choose an optimal tree size.
    + False, because we cannot directly calculate bias and variance without the knowledge of the true model. If we had the true model we still however we have to avoid double dipping so we could only use a training set to find it.

7. $\hat{Y}$ is an estimator for _______. Is it an unbiased estimator? Explain.
    + it's an estimator for E(Y) 
    + Y = f(x) = error \
      E(Y) = f(x) \
      E(error) = 0 \
      E($\hat{B}$) = B \
      $\hat{Y}$ = X$\hat{B}$ \ 
      E($\hat{Y}$) = E(X$\hat{B}$) = XB = E(Y)

8. Explain what (if anything) happens to a multiple linear regression model ($\hat{Y}$ = $\hat{B_{0}}$ + $\hat{B_{1}}$ $X_{1}$ + . . . $\hat{B_{p}}$$X_{p}$) under the following scenarios:
    a) We have a dataset where we have redundant information among the predictors.
        + if we have multicollinearity in the context of LSE our matrix $X^TX$ is no longer full rank due to columns being linear combinations of each other
        + to summarize, we can't take the inverse. least squares breaks down
    b) The response Y does not follow a normal distribution.
        + in order to make inference about the predicted value for y we need to assume that y follows a normal distribution
        + to summarize, nothing happing to model. can't make inference
    c) The assumption E($\epsilon_{i}$) = 0 does not hold.
        + everything is biased

9. Suppose you have a dataset with n = 1000 observations and p = 10,000 predictors. This is called the high-dimensional setting. For each technique, explain whether it is affected by the curse of dimensionality.
    a) Boosting
        + generally robust in high dimensional settings. Boosting can perform well even when the number of predictors is large. But the performance might degrade if there are many irrelevant or noisy predictors. 
    b) KNN classification
        + tends to be affected by the curse of dimensionality. As the number of predictors increase, the distance between data points become less meaningful, and all points may appear equidistant which can affect the performance of KNN.
    c) LDA
         + in high dimensional settings, estimating the covariance matrix accurately can be more challenging, especially when the number of predictors is close to or exceeds the number of observations. LDA dies, cannot take the inverse when n<p
    d) K-means clustering
        + tends to be affected by the curse of dimensionality. In high dimensional settings, the idea of distance can become less meaningful, and clusters can become less distinct. 
    e) Ridge regression
        + Can work well in high dimensional setting. It can handle multicollinearity. Penalty term helps overfitting when there are many predictors \
          $\hat{B} = (X^TX+ \lambda I)^{-1}X^TY$ 
          
10. Your colleague (who is inexperienced at machine learning) states that all your models are data-dependent. That means if you had used a different training set, your trained model would look different: your predictions for Y would change. This worries your colleague - how do they know which model is the ‘right’ one? What tools do you have at your disposal to address their concerns? Explain.
    + We have multiple tools at our disposal to alleviate the colleague's concerns. This is why we use a testing set. We are able to test the fit of our model on the testing set to see how well it performs. MSE is essentially just the squared difference between our predicted values and the actual values in the test set. A low test MSE shows that our model is performing well. We can also employ cross-validation. This allows us to take multiple splits of the training data to compare against the test data. This will help ensure that our model isn't hyper-specific to one training set but fits the data broadly as a whole. Another approach is an ensemble method like boosting. By combining multiple bootstrapped samples, we can reduce the variance, thus being confident that our model will not vary greatly based on changes to the training set.

11. In a well known competition for the Netflix, many teams decided to merge together before the competition finished for the one million dollar prize. Explain briefly what strategy the teams likely used to combine their algorithms, and why one can expect that this strategy will improve the final prediction results.
    + They likely used Ensemble to merge their multiple models. This improved the overall accuracy by reducing variance similar to what we have done for this class like bagging and boosting.


## Coding

1. Design a simulation study to calculate the variance of random forest as a function of m. Create a plot that shows that as m decreases, the variance of the bagged model decreases as well. Use simple statistical reasoning to explain why we see a reduction in variance as m decreases. You can use the same setup as HW 11, Problem 3.
    
    ```{r, echo=TRUE}
    library(randomForest)
    library(ISLR2)
    
    set.seed(1) # so we all get the same x values. 
    n = 100
    p = 20
    Xmat = matrix(NA,nrow=n,ncol=p)
    for(i in 1:p){
      Xmat[,i] = rnorm(n)
    }
    beta = rep(seq(1,3,length.out=5),4)
    Y = Xmat%*%beta + rnorm(n,0,1)
    
    train_set = data.frame(Xmat,Y)
    
    ### generate 1000 training sets and predicted values when all predictors = 1
    x0 = rep(1,20)
    x0 = as.data.frame(t(x0))
    colnames(x0) = colnames(train_set)[1:p]
    
    ### random forest 
    
    M = c(3,5,7,9,11,13,15,20)
    var = rep(NA,length(M))
    for(j in 1:length(M)){
      m = M[j]
      yhat1 = rep(NA,500)
      for(i in 1:500){
        Y = Xmat%*%beta + rnorm(n,0,1)
        data_sim = data.frame(Xmat,Y)
        bag.train = randomForest(Y~.,data=data_sim, mtry = m, importance = TRUE, ntree=200)
        yhat1[i] = predict(bag.train, newdata=x0)
        if(i%%100==0){print(i)}
      }
      Efhat1 = mean(yhat1)
      var[j] = mean((yhat1 - Efhat1)^2)
      print(var[j])
    }

    ```

2. We will use the NCI60 cancer cell line microarray data from library(ISLR2). This is also part of the ISLR2 library. Carry out the following pre-processing: 

      nci.labs = NCI60\$labs \
      nci.data = NCI60\$data
    
   a) Should we scale our features, which are gene expressions, in this setting? Justify your answer. If you decide to scale the features, do so.
      + Gene expression levels may have varying scales and scaling ensures that all features contribute equally.
   
    ```{r}
        library(ISLR2)
        nci.labs <- NCI60$labs
        nci.data <- NCI60$data
        scaled_nci_data = scale(nci.data)
    ```

    b) Implement K-means clustering on the (possibly scaled) data. Experiment with K = 2 and K = 4. Report the total within-cluster sum of squares for both K = 2 and K = 4.
    ```{r, echo=TRUE}
        kmeans_k2 = kmeans(scaled_nci_data, centers = 2, nstart = 20)  
        kmeans_k4 = kmeans(scaled_nci_data, centers = 4, nstart = 20)

        total_k2 = kmeans_k2$tot.withinss
        total_k4 = kmeans_k4$tot.withinss
        
        cat("Total within-cluster sum of squares for K = 2:", total_k2, "\nTotal within-cluster sum of squares for K = 4:", total_k4, "\n")
    ```

    c) Implement hierarchical clustering with complete linkage and Euclidean distance on the (possibly scaled) data. Cut the dendrogram to obtain 2 clusters. How does this compare to the K-means results we obtained in part (b) for K = 2? Report a confusion matrix to compare the results.
    ```{r, echo=TRUE}
        distM = dist(scaled_nci_data)
        hc.complete = hclust(distM, method="complete")
        h.clusters = cutree(hc.complete, 2)
        table(kmeans_k2$cluster, h.clusters)
    ```
    
    d) Discuss how you might validate your clustering results.
        + Since we have the true cluster lables, we can convert this to a supervised learning problem to test. You can split the data into a training and test set and perform k-fold validation on the training set. If the chosen method can consistently return the correct clusters, then that method would work well on new data
        
        
3. Implement PCA on the USArrests dataset. The proportion of variance explained by the first two principal components is 87%. Bootstrap the standard error for this quantity.
    ```{r, echo=TRUE}
    pr.out = prcomp(USArrests,scale=TRUE)
    sum(head(pr.out$sdev^2/sum(pr.out$sdev^2),2)) #86.75% of variance explained
    
    B = 2000
    n = dim(USArrests)[1]
    beta_0 = rep(0,2000)
    for(b in 1:B){
      index = sample(1:n,n,replace=TRUE)
      bootsample = USArrests[index,] 
      pr.out = prcomp(bootsample, scale = TRUE)
      fit = sum(head(pr.out$sdev^2/sum(pr.out$sdev^2),2))
      beta_0[b] = fit
    }
    sqrt(sum((beta_0-mean(beta_0))^2)/(B-1)) #Standard Error: ~0.022
    ```
