enet.pred = predict(enet.train,s=elastic_cv$lambda.min,newx=x[test,])
mean((enet.pred-Y.test)^2)
##############################################
## Problem 2: Regularized Regression Models ##
##############################################
################## 2a. ####################
library(ISLR2)
library(glmnet)
Hitters = na.omit(Hitters)
n = nrow(Hitters) #there are 263 observations
x = model.matrix(Salary ~.,data=Hitters)[,-1] #19 predictors
Y = Hitters$Salary
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test=(-train)
Y.test = Y[test]
################## 2b. ####################
grid = 10^seq(10, -2, length=100)
ridge.train = glmnet(x[train,], Y[train], alpha=0, lambda=grid)
set.seed(1)
cv.out = cv.glmnet(x[train,], Y[train], alpha=0, lambda=grid)
plot(cv.out)
ridgeMin = cv.out$lambda.min
ridgeMin
################## 2c. ####################
ridgelse = cv.out$lambda.1se
ridgelse
################## 2d. ####################
lasso.train = glmnet(x[train,],Y[train],alpha=1, lambda = grid)
set.seed(1)
cv.out.lasso = cv.glmnet(x[train,],Y[train],alpha = 1, lambda = grid)
plot(cv.out.lasso)
lassoMin = cv.out.lasso$lambda.min
lassoMin
################## 2e. ####################
lassolse = cv.out.lasso$lambda.1se
lassolse
################## 2f. ####################
ridge.min = predict(ridge.train, s=ridgeMin, newx = x[test,])
mean((ridge.min-Y.test)^2)
ridge.lse = predict(ridge.train, s=ridgelse, newx=x[test,])
mean((ridge.lse-Y.test)^2)
lasso.min = predict(lasso.train, s=lassoMin, newx = x[test,])
mean((lasso.min-Y.test)^2)
lasso.lse = predict(lasso.train, s=lassolse, newx = x[test,])
mean((lasso.lse-Y.test)^2)
################## 2g. ####################
coef(glmnet(x,Y,alpha=0, lambda = ridgeMin))
coef(glmnet(x,Y, alpha=0, lambda = ridgelse))
coef(glmnet(x,Y,alpha=1,lambda=lassoMin))
coef(glmnet(x,Y,alpha=1, lambda=lassolse))
################## 2h. ####################
alpha = seq(0.01, 0.99, by=0.01)
cv_error = rep(NA,length(alpha))
for (i in 1:length(alpha)){
set.seed(1)
cv_elastic = cv.glmnet(x[train,], Y[train], alpha = alpha[i], lambda=grid)
cv_error[i] = min(cv_elastic$cvm)
}
best_alpha = alpha[which.min(cv_error)]
best_alpha
## find the optimal lambda with best_alpha
set.seed(1)
elastic_cv = cv.glmnet(x[train,], Y[train], alpha = best_alpha,lambda=grid)
elastic_cv$lambda.min
################## 2i. ####################
enet.train = glmnet(x[train,],Y[train],alpha=best_alpha,lambda=grid)
enet.pred = predict(enet.train,s=elastic_cv$lambda.min,newx=x[test,])
mean((enet.pred-Y.test)^2)
coef(glmnet(x,Y,alpha=best_alpha, lambda = elastic_cv$lambda.min))
###############################
######### Problem 2 ###########
###############################
heart = read.table('https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data', sep=",",head=T,row.names=1)
######### Part 4 ###########
library(ggplot2)
library(gridExtra)
heart$famhist <- ifelse(heart$famhist == "Present", 1, 0)
par(mfrow=c(2,2))
plot(glm(chd~., data=heart, family='binomial'))
#############################################
### Problem 1: Multiple Linear Regression ###
#############################################
library(ISLR2)
plot(Boston)
plot(heart)
glm.fit = glm(chd ~ age  + famhist, data=heart,family='binomial')
summary(glm.fit)
glm.fit2 = glm(chd ~ age+famhist+tobacco, data=heart, family='binomial')
summary(glm.fit2)
# Create a contingency table of famhist and tobacco
contingency_table <- table(heart$famhist, heart$tobacco)
# Perform the chi-square test
chi_square_test <- chisq.test(contingency_table)
print(chi_square_test)
BIC(glm.fit2)
################## 1a. ####################
setwd("/Users/neham/Desktop/DS301")
getwd()
setwd("~/")
getwd()
setwd("./")
getwd()
setwd("C:/Users/neham/Desktop/DS303")
getwd()
setwd("C:/Users/neham/Desktop/DS301")
setwd("./")
setwd("C:/Users/neham/Desktop/DS301")
getwd()
getwd()
knitr::opts_chunk$set(echo = TRUE)
library(randomForest)
library(ISLR2)
set.seed(1) # so we all get the same x values.
n = 100
p = 20
Xmat = matrix(NA,nrow=n,ncol=p)
for(i in 1:p){
Xmat[,i] = rnorm(n)
}
beta = rep(seq(1,3,length.out=5),4)
Y = Xmat%*%beta + rnorm(n,0,1)
train_set = data.frame(Xmat,Y)
### generate 1000 training sets and predicted values when all predictors = 1
x0 = rep(1,20)
x0 = as.data.frame(t(x0))
colnames(x0) = colnames(train_set)[1:p]
### random forest
M = c(3,5,7,9,11,13,15,20)
var = rep(NA,length(M))
for(j in 1:length(M)){
m = M[j]
yhat1 = rep(NA,500)
for(i in 1:500){
Y = Xmat%*%beta + rnorm(n,0,1)
data_sim = data.frame(Xmat,Y)
bag.train = randomForest(Y~.,data=data_sim, mtry = m, importance = TRUE, ntree=200)
yhat1[i] = predict(bag.train, newdata=x0)
if(i%%100==0){print(i)}
}
Efhat1 = mean(yhat1)
var[j] = mean((yhat1 - Efhat1)^2)
print(var[j])
}
#############################
### Part 2: Random Forest ###
#############################
library(randomForest)
library(caret)
############# 2a. ################
heart = read.table('https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data', sep=",",head=T,row.names=1)
heart$chd <- as.factor(heart$chd)
possible_m <- c(2, 3, 4, 5)
# Create a function to perform 5-fold cross-validation
cv_error <- function(m) {
set.seed(1)
folds <- cut(seq(1, nrow(heart)), breaks=5, labels=FALSE)
cv_errors <- numeric(5)
for (i in 1:5) {
test_indices <- which(folds == i, arr.ind = TRUE)
train_data <- heart[-test_indices, ]
test_data <- heart[test_indices, ]
# Train the random forest model
rf_model <- randomForest(chd ~ ., data=train_data, mtry=m, ntree=500)
# Make predictions on the test set
predictions <- predict(rf_model, newdata=test_data)
# Calculate misclassification error
cv_errors[i] <- mean(predictions != test_data$chd)
}
# Return the average cross-validation error
return(mean(cv_errors))
}
# Perform cross-validation for each value of m
cv_errors <- sapply(possible_m, cv_error)
# Find the optimal m with the minimum cross-validation error
optimal_m <- possible_m[which.min(cv_errors)]
# Report the 5-fold CV error for each m
cat("5-Fold CV Error for each m:", cv_errors, "\n")
# Report the optimal m
cat("Optimal m:", optimal_m, "\n")
# Load the required library
library(randomForest)
# Set the seed for reproducibility
set.seed(1)
# Read the dataset
heart <- read.table('https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data', sep=",", head=TRUE, row.names=1)
# Convert chd to a factor
heart$chd <- as.factor(heart$chd)
# Specify the possible values for m
possible_m <- c(2, 3, 4, 5)
# Split the data into training and test sets
train_data <- heart[101:nrow(heart), ]
test_data <- heart[1:100, ]
# Create a function to perform 5-fold cross-validation
cv_error <- function(m) {
set.seed(1)
cv_errors <- numeric(5)
for (i in 1:5) {
# Use the same training data for each fold
# Test set is the first 100 observations
test_indices <- seq((i - 1) * 20 + 1, i * 20)
train_fold <- train_data[-test_indices, ]
test_fold <- train_data[test_indices, ]
# Train the random forest model
rf_model <- randomForest(chd ~ ., data=train_fold, mtry=m, ntree=500)
# Make predictions on the test set
predictions <- predict(rf_model, newdata=test_fold)
# Calculate misclassification error
cv_errors[i] <- mean(predictions != test_fold$chd)
}
# Return the average cross-validation error
return(mean(cv_errors))
}
# Perform cross-validation for each value of m
cv_errors <- sapply(possible_m, cv_error)
# Find the optimal m with the minimum cross-validation error
optimal_m <- possible_m[which.min(cv_errors)]
# Report the 5-fold CV error for each m
cat("5-Fold CV Error for each m:", cv_errors, "\n")
# Report the optimal m
cat("Optimal m:", optimal_m, "\n")
#############################
### Part 2: Random Forest ###
#############################
library(randomForest)
library(caret)
############# 2a. ################
heart = read.table('https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data', sep=",",head=T,row.names=1)
set.seed(1)
heart$chd = as.factor(heart$chd)
possible_m = c(2, 3, 4, 5)
train_data = heart[101:nrow(heart), ]
test_data = heart[1:100, ]
cv_error <- function(m) {
set.seed(1)
cv_errors <- numeric(5)
for (i in 1:5) {
# attempt to prevent double dipping
test_indices <- seq((i - 1) * 20 + 1, i * 20)
train_fold <- train_data[-test_indices, ]
test_fold <- train_data[test_indices, ]
rf_model = randomForest(chd ~ ., data=train_fold, mtry=m, ntree=500)
predictions = predict(rf_model, newdata=test_fold)
cv_errors[i] = mean(predictions != test_fold$chd)
}
return(mean(cv_errors))
}
# Perform cross-validation for each value of m
cv_errors = sapply(possible_m, cv_error)
cv_errors
optimal_m = possible_m[which.min(cv_errors)]
optimal_m
############# 2b. ################
optimal_m = 3
rf_model = randomForest(chd ~ ., data=train_data, mtry=optimal_m, ntree=500)
predictions = predict(rf_model, newdata=test_data)
misclassification_rate = mean(predictions != test_data$chd)
# Report the misclassification rate
cat("Misclassification Rate on Test Set:", misclassification_rate, "\n")
############# 2c. ################
rf_model_full <- randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
# Obtain the OOB misclassification error
oob_error <- 1 - rf_model_full$err.rate[nrow(rf_model_full$err.rate), "OOB"]
oob_error
############# 2d. ################
observation_2 = heart[2, -ncol(heart)]
# Predict the probability of Y=1 for the 2nd observation
probability_Y1 = predict(rf_model_full, newdata=observation_2, type='response')
# Report the probability
cat("Phat(Y=1|X) for the 2nd observation:", probability_Y1, "\n")
rf_model <- randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
oob_predictions <- predict(rf_model, newdata=heart, type="response", predict.all=TRUE)$individual
oob_predictions_final <- rep(NA, nrow(heart))
for (i in 1:nrow(heart)) {
oob_predictions_final[i] <- names(table(oob_predictions[i, rf_model$inbag[i,] == 0]))[which.max(table(oob_predictions[i, rf_model$inbag[i,] == 0]))]
}
for (i in 1:nrow(heart)) {
oob_predictions_final[i] <- names(table(oob_predictions[i, , drop = FALSE][rf_model$inbag[i, , drop = FALSE] == 0]))[which.max(table(oob_predictions[i, , drop = FALSE][rf_model$inbag[i, , drop = FALSE] == 0]))]
}
#############################
### Part 2: Random Forest ###
#############################
library(randomForest)
library(caret)
############# 2a. ################
heart = read.table('https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data', sep=",",head=T,row.names=1)
set.seed(1)
heart$chd = as.factor(heart$chd)
possible_m = c(2, 3, 4, 5)
train_data = heart[101:nrow(heart), ]
test_data = heart[1:100, ]
cv_error <- function(m) {
set.seed(1)
cv_errors <- numeric(5)
for (i in 1:5) {
# attempt to prevent double dipping
test_indices <- seq((i - 1) * 20 + 1, i * 20)
train_fold <- train_data[-test_indices, ]
test_fold <- train_data[test_indices, ]
rf_model = randomForest(chd ~ ., data=train_fold, mtry=m, ntree=500)
predictions = predict(rf_model, newdata=test_fold)
cv_errors[i] = mean(predictions != test_fold$chd)
}
return(mean(cv_errors))
}
# Perform cross-validation for each value of m
cv_errors = sapply(possible_m, cv_error)
cv_errors
optimal_m = possible_m[which.min(cv_errors)]
optimal_m
############# 2b. ################
optimal_m = 3
rf_model = randomForest(chd ~ ., data=train_data, mtry=optimal_m, ntree=500)
predictions = predict(rf_model, newdata=test_data)
misclassification_rate = mean(predictions != test_data$chd)
misclassification_rate
############# 2c. ################
rf_model_full = randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
rf_model <- randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
oob_predictions <- predict(rf_model, newdata=heart, type="response", predict.all=TRUE)$individual
oob_predictions_final <- rep(NA, nrow(heart))
for (i in 1:nrow(heart)) {
oob_predictions_final[i] <- names(table(oob_predictions[i, , drop = FALSE][rf_model$inbag[i, , drop = FALSE] == 0]))[which.max(table(oob_predictions[i, , drop = FALSE][rf_model$inbag[i, , drop = FALSE] == 0]))]
}
for (i in 1:nrow(heart)) {
oob_predictions_final[i] <- names(table(oob_predictions[i, rf_model$inbag[i, ] == 0]))[which.max(table(oob_predictions[i, rf_model$inbag[i, ] == 0]))]
}
############# 2c. ################
rf_model_full = randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
# Obtain OOB predictions
oob_predictions <- predict(rf_model_full, newdata=heart, type="response", predict.all=TRUE)$individual
# Initialize a vector for OOB predictions
oob_predictions_final <- rep(NA, nrow(heart))
# Assign OOB predictions based on majority class
for (i in 1:nrow(heart)) {
oob_predictions_final[i] <- names(table(oob_predictions[i, , drop = FALSE][rf_model_full$inbag[i, , drop = FALSE] == 0]))[which.max(table(oob_predictions[i, , drop = FALSE][rf_model_full$inbag[i, , drop = FALSE] == 0]))]
}
# Calculate OOB misclassification error
oob_error <- mean(oob_predictions_final != heart$chd)
# Report the OOB misclassification error
cat("OOB Misclassification Error:", oob_error, "\n")
oob_predictions <- predict(rf_model_full, type = "response")
# Calculate OOB misclassification error
oob_error <- mean(oob_predictions != heart$chd)
# Report the OOB misclassification error
cat("OOB Misclassification Error:", oob_error, "\n")
###testing
rf_model <- randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
# Get individual predictions for each observation
all_pred <- predict(rf_model, newdata=heart, predict.all=TRUE)$individual
# Initialize OOB predictions vector
oob_predictions <- rep(NA, nrow(heart))
# Populate OOB predictions using out-of-bag samples
for (i in 1:nrow(heart)) {
oob_predictions[i] <- names(table(all_pred[i, rf_model$inbag[i,] == 0]))[which.max(table(all_pred[i, rf_model$inbag[i,] == 0]))]
}
# Initialize OOB predictions vector
oob_predictions <- rep(NA, nrow(heart))
# Populate OOB predictions using out-of-bag samples
for (i in 1:nrow(heart)) {
oob_inbag_indices <- which(rf_model$inbag[i, ] == 0)
if (length(oob_inbag_indices) > 0) {
oob_predictions[i] <- names(table(all_pred[i, oob_inbag_indices]))[which.max(table(all_pred[i, oob_inbag_indices]))]
} else {
# Handle the case where there are no out-of-bag samples
oob_predictions[i] <- NA
}
}
# Remove NA values before calculating OOB misclassification error
oob_predictions <- oob_predictions[!is.na(oob_predictions)]
# Calculate OOB misclassification error
oob_error <- mean(oob_predictions != heart$chd)
# Report the OOB misclassification error
cat("OOB Misclassification Error:", oob_error, "\n")
for (i in 1:nrow(heart)) {
inbag_indices <- which(rf_model$inbag[i,] == 0)
if (length(inbag_indices) > 0) {
oob_predictions[i] <- names(table(all_pred[i, inbag_indices]))[which.max(table(all_pred[i, inbag_indices]))]
} else {
oob_predictions[i] <- NA
}
}
oob_predictions <- oob_predictions[!is.na(oob_predictions)]
# Calculate OOB misclassification error
oob_error <- mean(oob_predictions != heart$chd)
# Report the OOB misclassification error
cat("OOB Misclassification Error:", oob_error, "\n")
#### from homework
rf_model_full = randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
all_pred = predict(rf_model_full, newdata = heart, predict.all = TRUE)$individual
oob_predictions = rep(NA, nrow(heart))
for (i in 1:nrow(heart)) {
oob_predictions[i] = names(table(all_pred[i, rf_model_full$inbag[i,] == 0]))[which.max(table(all_pred[i, rf_model_full$inbag[i,] == 0]))]
}
oob_predictions = rep(NA, nrow(heart))
for (i in 1:nrow(heart)) {
inbag_indices = which(rf_model_full$inbag[i,] == 0)
if (length(inbag_indices) > 0) {
all_pred_i = all_pred[i, inbag_indices]
oob_predictions[i] = names(table(all_pred_i))[which.max(table(all_pred_i))]
} else {
# Handle the case where there are no out-of-bag observations
oob_predictions[i] = NA
}
}
oob_error = mean(oob_predictions != heart$chd)
oob_error
#############################
### Part 2: Random Forest ###
#############################
library(randomForest)
library(caret)
############# 2a. ################
heart = read.table('https://hastie.su.domains/ElemStatLearn/datasets/SAheart.data', sep=",",head=T,row.names=1)
set.seed(1)
heart$chd = as.factor(heart$chd)
possible_m = c(2, 3, 4, 5)
train_data = heart[101:nrow(heart), ]
test_data = heart[1:100, ]
cv_error <- function(m) {
set.seed(1)
cv_errors <- numeric(5)
for (i in 1:5) {
# attempt to prevent double dipping
test_indices <- seq((i - 1) * 20 + 1, i * 20)
train_fold <- train_data[-test_indices, ]
test_fold <- train_data[test_indices, ]
rf_model = randomForest(chd ~ ., data=train_fold, mtry=m, ntree=500)
predictions = predict(rf_model, newdata=test_fold)
cv_errors[i] = mean(predictions != test_fold$chd)
}
return(mean(cv_errors))
}
# Perform cross-validation for each value of m
cv_errors = sapply(possible_m, cv_error)
cv_errors
optimal_m = possible_m[which.min(cv_errors)]
optimal_m
############# 2b. ################
optimal_m = 3
rf_model = randomForest(chd ~ ., data=train_data, mtry=optimal_m, ntree=500)
predictions = predict(rf_model, newdata=test_data)
misclassification_rate = mean(predictions != test_data$chd)
misclassification_rate
############# 2c. ################
rf_model_full = randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
#### from homework
rf_model_full = randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500)
all_pred = predict(rf_model_full, newdata = heart, predict.all = TRUE)$individual
oob_predictions = rep(NA, nrow(heart))
for (i in 1:nrow(heart)) {
oob_predictions[i] = names(table(all_pred[i, rf_model_full$inbag[i,] == 0]))[which.max(table(all_pred[i, rf_model_full$inbag[i,] == 0]))]
}
oob_predictions = rep(NA, nrow(heart))
for (i in 1:nrow(heart)) {
inbag_indices = which(rf_model_full$inbag[i,] == 0)
if (length(inbag_indices) > 0) {
all_pred_i = all_pred[i, inbag_indices]
oob_predictions[i] = names(table(all_pred_i))[which.max(table(all_pred_i))]
} else {
# Handle the case where there are no out-of-bag observations
oob_predictions[i] = NA
}
}
# Exclude NA values before calculating the mean
oob_error = mean(oob_predictions[!is.na(oob_predictions)] != heart$chd)
oob_error
#### from homework
rf_model_full = randomForest(chd ~ ., data=heart, mtry=optimal_m, ntree=500, importance=TRUE, keep.inbag=TRUE)
all_pred = predict(rf_model_full, newdata = heart, predict.all = TRUE)$individual
oob_predictions = rep(NA, nrow(heart))
for (i in 1:nrow(heart)) {
oob_predictions[i] = names(table(all_pred[i, rf_model_full$inbag[i,] == 0]))[which.max(table(all_pred[i, rf_model_full$inbag[i,] == 0]))]
}
oob_error = mean(oob_predictions != Carseats$High)
oob_error
oob_error = mean(oob_predictions != heart$chd)
oob_error
# Obtain the probability for the 2nd observation
prob_y1_second_obs <- all_pred[2, , drop=FALSE]
# Report the probability
cat("Phat(Y=1|X) for the 2nd observation:", prob_y1_second_obs, "\n")
############# 2d. ################
observation_2 = heart[2, -ncol(heart)]
probability_Y1 = predict(rf_model_full, newdata=observation_2, type='response')
probability_Y1
probability_Y1 = predict(rf_model_full, newdata=observation_2, type='predict')
probability_Y1
probability_Y1 = predict(rf_model_full, newdata=observation_2, type='prob')
probability_Y1
############# 2d. ################
observation_2 = heart[2, -ncol(heart)]
probability_Y1 = predict(rf_model_full, newdata=observation_2, type='prob')
probability_Y1
############# 2e. ################
bootstrap_estimates <- numeric(1000)
num_trees_bootstrap <- 25
for (i in 1:1000) {
# Sample with replacement from the entire dataset
bootstrap_sample <- heart[sample(nrow(heart), replace=TRUE), ]
# Train the random forest model on the bootstrap sample
rf_model_bootstrap <- randomForest(chd ~ ., data=bootstrap_sample, mtry=optimal_m, ntree=num_trees_bootstrap, importance=TRUE, keep.inbag=TRUE)
# Extract individual predictions for the 2nd observation
observation_2 <- bootstrap_sample[2, -ncol(bootstrap_sample)]
probability_Y1_bootstrap <- predict(rf_model_bootstrap, newdata=observation_2, type='response')
# Store the estimate in the vector
bootstrap_estimates[i] <- probability_Y1_bootstrap
}
# Calculate the standard error of bootstrap estimates
se_bootstrap <- sd(bootstrap_estimates)
# Report the standard error
cat("Bootstrap Standard Error of Phat(Y=1|X) for the 2nd observation:", se_bootstrap, "\n")
