tree_model = tree(Y ~ ., data = X)
# Create a data frame with all X's equal to 1
new_data = data.frame(matrix(1, nrow = 1, ncol = ncol(X)))
colnames(new_data) = colnames(X)
predictions = predict(tree_model, newdata = new_data)
return(predictions)
}
num_simulations = 1000
# Store predicted values for the first 5 iterations
first_five_predictions = matrix(NA, nrow = 5, ncol = num_simulations)
# Simulate 1000 training sets and train decision trees
for (iteration in 1:num_simulations) {
n = 100
p = 20
Xmat = matrix(NA, nrow = n, ncol = p)
for (i in 1:p) {
Xmat[, i] = rnorm(n)
}
beta = rep(seq(1, 3, length.out = 5), 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
train_set = data.frame(Xmat, Y)
predictions = fit_tree_and_predict(train_set[, -p], train_set$Y)
if (iteration <= 5) {
first_five_predictions[iteration, ] <- predictions
}
}
print("First predicted values for the first 5 iterations:")
print(first_five_predictions)
set.seed(1)
# Function to fit a decision tree and return predicted values when X1 = 1
fit_tree_and_predict <- function(train_set) {
library(tree)
# Fit a decision tree (overfit, no pruning)
tree_model <- tree(Y ~ ., data = train_set)
# Create a new data frame with X1 = 1
new_data <- data.frame(X1 = 1, matrix(1, ncol = p - 1, nrow = 1))
# Predict values when X1 = 1
predictions <- predict(tree_model, newdata = new_data)
return(predictions)
}
n_iterations <- 1000
first_predictions <- matrix(NA, nrow = 5, ncol = n_iterations)
for (iteration in 1:n_iterations) {
# Generate X values
Xmat = matrix(rnorm(n * p), nrow = n, ncol = p)
# Generate Y values
beta = rep(seq(1, 3, length.out = 5), each = 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
# Create training set
train_set = data.frame(Xmat, Y)
# Fit decision tree and store predicted values
predictions <- fit_tree_and_predict(train_set)
# Store the first 5 predicted values for each iteration
first_predictions[, iteration] <- predictions[1:5]
}
# Initialize an empty vector to store the predicted values
predicted_values <- numeric(1000)
# Set seed for reproducibility
set.seed(1)
# Number of simulations
num_simulations <- 1000
# Loop over the simulations
for (iteration in 1:num_simulations) {
# Simulate training set
n = 100
p = 20
Xmat = matrix(NA, nrow = n, ncol = p)
for (i in 1:p) {
Xmat[, i] = rnorm(n)
}
beta = rep(seq(1, 3, length.out = 5), 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
train_set = data.frame(Xmat, Y)
# Train decision tree (overfit, no pruning)
library(tree)
tree_model <- tree(Y ~ ., data = train_set)
# Create a data frame with X values all equal to 1
new_data <- data.frame(matrix(1, nrow = 1, ncol = p))
names(new_data) <- names(train_set)[-p]  # Set column names
# Predict for X values all equal to 1
predicted_value <- predict(tree_model, newdata = new_data)
# Store the predicted value
predicted_values[iteration] <- predicted_value
# Print the first 5 predicted values for the first 5 iterations
if (iteration <= 5) {
cat("Iteration", iteration, "- Predicted Value:", predicted_value, "\n")
}
}
# Initialize an empty vector to store the predicted values
predicted_values <- numeric(1000)
# Set seed for reproducibility
set.seed(1)
# Number of simulations
num_simulations <- 1000
# Loop over the simulations
for (iteration in 1:num_simulations) {
# Simulate training set
n = 100
p = 20
Xmat = matrix(NA, nrow = n, ncol = p)
for (i in 1:p) {
Xmat[, i] = rnorm(n)
}
beta = rep(seq(1, 3, length.out = 5), 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
train_set = data.frame(Xmat, Y)
# Train decision tree (overfit, no pruning)
library(tree)
tree_model <- tree(Y ~ ., data = train_set)
# Create a data frame with X values all equal to 1
new_data <- data.frame(matrix(1, nrow = 1, ncol = p))
names(new_data) <- names(train_set)[-p]  # Set column names
# Predict for X values all equal to 1
predicted_value <- predict(tree_model, newdata = new_data)
# Store the predicted value
predicted_values[iteration] <- predicted_value
# Print the first 5 predicted values for the first 5 iterations
if (iteration <= 5) {
cat("Iteration", iteration, "- Predicted Value:", predicted_value, "\n")
}
}
# Initialize an empty vector to store the predicted values
predicted_values <- numeric(1000)
# Set seed for reproducibility
set.seed(1)
# Number of simulations
num_simulations <- 1000
# Loop over the simulations
for (iteration in 1:num_simulations) {
# Simulate training set
n = 100
p = 20
Xmat = matrix(NA, nrow = n, ncol = p)
for (i in 1:p) {
Xmat[, i] = rnorm(n)
}
beta = rep(seq(1, 3, length.out = 5), 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
train_set = data.frame(Xmat, Y)
# Train decision tree (overfit, no pruning)
library(tree)
tree_model <- tree(Y ~ ., data = train_set)
# Create a data frame with X values all equal to 1
new_data <- data.frame(matrix(1, nrow = 1, ncol = p))
names(new_data) <- names(train_set)[-length(names(train_set))]  # Exclude the response variable
# Predict for X values all equal to 1
predicted_value <- predict(tree_model, newdata = new_data)
# Store the predicted value
predicted_values[iteration] <- predicted_value
# Print the first 5 predicted values for the first 5 iterations
if (iteration <= 5) {
cat("Iteration", iteration, "- Predicted Value:", predicted_value, "\n")
}
}
# Report the first predicted values for the first 5 iterations
cat("\nFirst predicted values for the first 5 iterations:\n")
cat(predicted_values[1:5], "\n")
library(tree)
# Initialize an empty vector to store the predicted values
predicted_values <- numeric(1000)
# Set seed for reproducibility
set.seed(1)
# Number of simulations
num_simulations <- 1000
# Loop over the simulations
for (iteration in 1:num_simulations) {
# Simulate training set
n = 100
p = 20
Xmat = matrix(NA, nrow = n, ncol = p)
for (i in 1:p) {
Xmat[, i] = rnorm(n)
}
beta = rep(seq(1, 3, length.out = 5), 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
train_set = data.frame(Xmat, Y)
# Train decision tree (overfit, no pruning)
library(tree)
tree_model <- tree(Y ~ ., data = train_set)
# Create a data frame with X values all equal to 1
new_data <- data.frame(matrix(1, nrow = 1, ncol = p))
names(new_data) <- names(train_set)[-length(names(train_set))]  # Exclude the response variable
# Predict for X values all equal to 1
predicted_value <- predict(tree_model, newdata = new_data)
# Store the predicted value
predicted_values[iteration] <- predicted_value
# Print the first 5 predicted values for the first 5 iterations
if (iteration <= 5) {
cat("Iteration", iteration, "- Predicted Value:", predicted_value, "\n")
}
}
# Report the first predicted values for the first 5 iterations
cat("\nFirst predicted values for the first 5 iterations:\n")
cat(predicted_values[1:5], "\n")
##############################
## Problem 3: Bias of Trees ##
##############################
################## 3a. ####################
library(tree)
predicted_values = numeric(1000)
set.seed(1)
num_simulations = 1000
# Loop over the simulations
for (iteration in 1:num_simulations) {
# Simulate training set
n = 100
p = 20
Xmat = matrix(NA, nrow = n, ncol = p)
for (i in 1:p) {
Xmat[, i] = rnorm(n)
}
beta = rep(seq(1, 3, length.out = 5), 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
train_set = data.frame(Xmat, Y)
# Train decision tree (overfit, no pruning)
tree_model = tree(Y ~ ., data = train_set)
# Create a data frame with X values all equal to 1
new_data = data.frame(matrix(1, nrow = 1, ncol = p))
names(new_data) = names(train_set)[-length(names(train_set))]  # Exclude the response variable
# Predict for X values all equal to 1
predicted_value = predict(tree_model, newdata = new_data)
predicted_values[iteration] <- predicted_value
}
cat(predicted_values[1:5], "\n")
################## 3b. ####################
true_value = mean(rep(seq(1, 3, length.out = 5), 4))
bias_squared = mean((predicted_values - true_value)^2)
variance = mean((predicted_values - mean(predicted_values))^2)
bias_squared
variance
################## 3c. ####################
library(randomForest)
rf_predicted_values = numeric(1000)
set.seed(1)
num_trees = 200
for (iteration in 1:num_simulations) {
n = 100
p = 20
Xmat = matrix(NA, nrow = n, ncol = p)
for (i in 1:p) {
Xmat[, i] = rnorm(n)
}
beta = rep(seq(1, 3, length.out = 5), 4)
Y = Xmat %*% beta + rnorm(n, 0, 1)
train_set = data.frame(Xmat, Y)
# Train random forest
rf_model = randomForest(Y ~ ., data = train_set, mtry = 10, ntree = num_trees)
# Create a data frame with X values all equal to 1
new_data = data.frame(matrix(1, nrow = 1, ncol = p))
names(new_data) = names(train_set)[-length(names(train_set))]  # Exclude the response variable
# Predict for X values all equal to 1
rf_predicted_value = predict(rf_model, newdata = new_data)
# Store the predicted value for random forests
rf_predicted_values[iteration] = rf_predicted_value
}
cat(rf_predicted_values[1:5], "\n")
rf_predicted_values[1:5]
################## 3d. ####################
true_value = mean(rep(seq(1, 3, length.out = 5), 4))
rf_bias_squared = mean((rf_predicted_values - true_value)^2)
rf_variance = mean((rf_predicted_values - mean(rf_predicted_values))^2)
rf_bias_squared
rf_variance
################## 3e. ####################
change_in_order_of_magnitude = log10(rf_bias_squared / bias_squared)
change_in_order_of_magnitude
change_in_order_of_magnitude_variance <- log10(rf_variance / variance)
change_in_order_of_magnitude_variance
################## 3e. ####################
change_in_order_of_magnitude_bias = log10(rf_bias_squared / bias_squared)
change_in_order_of_magnitude_bias
################## 3f. ####################
change_in_order_of_magnitude_variance <- log10(rf_variance / variance)
change_in_order_of_magnitude_variance
######################################
## Problem 4: Understanding K-Means ##
######################################
################## 4c.i. ####################
x = cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
################## 4c.ii. ####################
set.seed(1)
labels = sample(2, nrow(x), replace = T)
labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
################## 4c.iii. ####################
centroid1 = c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 = c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
centroid1
centroid2
1/4(0 + 1 + 4 + 5)
1/4*(0 + 1 + 4 + 5)
1/4*(4 + 4 + 0 + 1)
################## 4c.iv. ####################
labels <- c(1, 1, 1, 1, 2, 2)
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
######################################
## Problem 4: Understanding K-Means ##
######################################
################## 4c.i. ####################
x = cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
################## 4c.ii. ####################
set.seed(1)
labels = sample(2, nrow(x), replace = T)
labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
################## 4c.iii. ####################
centroid1 = c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 = c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
centroid1
centroid2
# Run k-means clustering
kmeans_result = kmeans(x, centers = 2, nstart = 20)
# Get cluster labels
cluster_labels = kmeans_result$cluster
# Print cluster labels
cat("Cluster Labels:", cluster_labels, "\n")
# Plot the data with cluster assignments
plot(x[, 1], x[, 2], col = (cluster_labels + 1), pch = 20, cex = 2)
# Plot centroids
points(kmeans_result$centers[, 1], kmeans_result$centers[, 2], col = 2:3, pch = 4)
# Run k-means clustering
kmeans_result = kmeans(x, centers = 2, nstart = 20)
# Get cluster labels
cluster_labels = kmeans_result$cluster
# Print cluster labels
cat("Cluster Labels:", cluster_labels, "\n")
# Plot the data with cluster assignments
plot(x[, 1], x[, 2], col = (cluster_labels + 1), pch = 20, cex = 2)
# Plot centroids
points(kmeans_result$centers[, 1], kmeans_result$centers[, 2], col = 2:3, pch = 4)
# Plot green "x" symbols
points(x[, 1], x[, 2], col = ifelse(cluster_labels == 1, 4, 5), pch = "x", cex = 2)
################## 4c.i. ####################
x = cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
################## 4c.ii. ####################
set.seed(1)
labels = sample(2, nrow(x), replace = T)
labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
################## 4c.iii. ####################
centroid1 = c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 = c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
################## 4c.iv. ####################
get_closest_centroid <- function(point, centroids) {
distances <- apply(centroids, 1, function(c) sqrt(sum((point - c)^2)))
return(which.min(distances))
}
for (i in 1:nrow(x)) {
labels[i] <- get_closest_centroid(x[i,], rbind(centroid1, centroid2)) + 1
}
# Display cluster labels
labels
# Plot with new cluster labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
################## 4c.iv. ####################
get_closest_centroid <- function(point, centroids) {
distances <- apply(centroids, 1, function(c) sqrt(sum((point - c)^2)))
return(which.min(distances))
}
for (i in 1:nrow(x)) {
labels[i] <- get_closest_centroid(x[i,], rbind(centroid1, centroid2))
}
# Display cluster labels
labels
# Plot with new cluster labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
######################################
## Problem 4: Understanding K-Means ##
######################################
################## 4c.i. ####################
x = cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
################## 4c.ii. ####################
set.seed(1)
labels = sample(2, nrow(x), replace = T)
labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
################## 4c.iii. ####################
centroid1 = c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 = c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
################## 4c.iv. ####################
get_closest_centroid <- function(point, centroids) {
distances <- apply(centroids, 1, function(c) sqrt(sum((point - c)^2)))
return(which.min(distances))
}
for (i in 1:nrow(x)) {
labels[i] <- get_closest_centroid(x[i,], rbind(centroid1, centroid2))
}
# Display cluster labels
labels
get_closest_centroid <- function(point, centroids) {
distances <- apply(centroids, 1, function(c) sqrt(sum((point - c)^2)))
return(which.min(distances))
}
# Update labels until convergence
converged <- FALSE
while (!converged) {
new_labels <- sapply(1:nrow(x), function(i) get_closest_centroid(x[i,], rbind(centroid1, centroid2)) + 1)
if(all(new_labels == labels)) {
converged <- TRUE
} else {
labels <- new_labels
centroid1 = c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 = c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
}
}
################## 4c.iv. ####################
labels <- c(1, 1, 1, 1, 2, 2)
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
######################################
## Problem 4: Understanding K-Means ##
######################################
################## 4c.i. ####################
x = cbind(c(1, 1, 0, 5, 6, 4), c(4, 3, 4, 1, 2, 0))
plot(x[,1], x[,2])
################## 4c.ii. ####################
set.seed(1)
labels = sample(2, nrow(x), replace = T)
labels
plot(x[, 1], x[, 2], col = (labels + 1), pch = 20, cex = 2)
################## 4c.iii. ####################
centroid1 = c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 = c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
################## 4c.iv. ####################
# Function to calculate Euclidean distance between two points
euclidean_distance <- function(point1, point2) {
sqrt(sum((point1 - point2)^2))
}
# Assign each observation to the closest centroid
for (i in 1:nrow(x)) {
distance_to_centroid1 <- euclidean_distance(x[i, ], centroid1)
distance_to_centroid2 <- euclidean_distance(x[i, ], centroid2)
# Assign the observation to the cluster with the closest centroid
if (distance_to_centroid1 < distance_to_centroid2) {
labels[i] <- 1
} else {
labels[i] <- 2
}
}
# Report the updated cluster labels
cat("Updated cluster labels:", labels, "\n")
# Update the plot
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
################## 4c.v. ####################
centroid1 = c(mean(x[labels == 1, 1]), mean(x[labels == 1, 2]))
centroid2 = c(mean(x[labels == 2, 1]), mean(x[labels == 2, 2]))
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
centroid1
centroid2
euclidean_distance <- function(point1, point2) {
sqrt(sum((point1 - point2)^2))
}
for (i in 1:nrow(x)) {
distance_to_centroid1 <- euclidean_distance(x[i, ], centroid1)
distance_to_centroid2 <- euclidean_distance(x[i, ], centroid2)
# Assign the observation to the cluster with the closest centroid
if (distance_to_centroid1 < distance_to_centroid2) {
labels[i] <- 1
} else {
labels[i] <- 2
}
}
lables
euclidean_distance <- function(point1, point2) {
sqrt(sum((point1 - point2)^2))
}
for (i in 1:nrow(x)) {
distance_to_centroid1 <- euclidean_distance(x[i, ], centroid1)
distance_to_centroid2 <- euclidean_distance(x[i, ], centroid2)
# Assign the observation to the cluster with the closest centroid
if (distance_to_centroid1 < distance_to_centroid2) {
labels[i] <- 1
} else {
labels[i] <- 2
}
}
labels
plot(x[,1], x[,2], col=(labels + 1), pch = 20, cex = 2)
points(centroid1[1], centroid1[2], col = 2, pch = 4)
points(centroid2[1], centroid2[2], col = 3, pch = 4)
################## 4c.vi. ####################
plot(x[, 1], x[, 2], col=(labels + 1), pch = 20, cex = 2)
